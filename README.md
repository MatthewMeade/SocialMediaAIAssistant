# Social Media Calendar: AI Assistant
An AI enabled social media planning tool for social media managers to create content aligned with their brand and audience

## Decoupled and Modular AI Architecture

* The AI service is built as a modular system built to fit into an existing app
    * Each part of the AI service can be easily swapped out, with minimal lock-in
* The client interacts with this service directly, the service interacts with the app's normal backend on behalf of the user
* Written in TypeScript
* **Component Roles:**
    * **Frontend:** Stateful React application managing view state, rendering, and client-side routing.
    * **AI Service:** Stateless API handling all AI based functionality, including the chatbot
        * **Vector DB:** Indexes private user data and general knowledgeable for search
    * **API Backend:** The main app's CRUD routes, DB, and auth
    

## Dynamic Context Injection

* Runtime injection of UI state into the System Prompt
* **Context Tracking:** Frontend manager tracking the user's current state
    * **Page/Component:** Identifies current view (e.g., `CalendarPage`, `PostEditor`).
    * **Active Entities:** Resolves IDs for currently open items (e.g., `activePostId`, `selectedNoteId`).
* **Retrieval:** Enables implicit RAG without conversational disambiguation
   * **Searches Vector DB**: Retrieves relevant info from the user's private data along with universal sources
   * **Loads Entities**: Loads data relevant to what's on the user's screen, for example the current post 
   * **App knowledge**: Dynamic information for the AI based on the user's state
     * Key workflows (how to create a post)
     * Available tools and features

## RAG Security - Core App

* The AI service has the same read access as the user
    * It is not possible to prompt the AI to trick it into querying for other user's data
* Requests to the AI service from the client include a signed token identifying the user
* Tool calls and RAG steps use this verified user information directly from the session, data like the user ID are never generated by the LLM 
* The actual token is passed through to the downstream service which handles authorization

## RAG Security - Vector DB

* The AI service does manage the vector DB for indexing itself
* The main app sends events when records are created or updated to be indexed
* For private data, all stored data includes the account ID as metadata
* When searching for data, the user's validated account ID is used to filter results
* As an extra layer of security, where possible the retrieval step should use the retrieved record's ID to do an authenticated read for the latest data

## Client Side Tool Calls 

* The AI service has read only access to the application, it can't overwrite anything
* When a tool configured as client side is called, agent execution halts and the current state including the tool data is returned to the user
* The client renders the tool call for approval - there is always a human in the loop
* When the tool call is approved, an event is fired which triggers the action in the app
    * Eg: Update the post content
    * Event bus decouples chat UI from the app UI
* The tool handler watches the client state to confirm the action was completed
* A tool call success message is automatically sent to the AI service to resume execution

## Server Side Tool Calls

* All of the app's AI capabilities are exposed the AI as tool calls
* The tools are also accessible by API endpoints to be called directly from the client
* Non-agentic use cases, such caption generation and content grading
* Tools are also used inside other tools, for example the caption generator uses the brand voice score tool to evaluate the generated caption and re-generate with the returned feedback if the score is low

## Guardrails

* To ensure the user's query is appropriate, the guardrails middleware will block anything it determines the chatbot shouldn't be able to help with

## Streaming

* Custom Server-Sent Events (SSE) implementation.
* Event types to update the UI with status updates, improving perceived performance
    * `[TOKEN]`: Text generation buffer.
    * `[STATUS]`: System state indicators (e.g., "Querying Vector Store...").
    * `[CONTROL]`: Directives for input locking or context clearing.
* Using a secondary channel simplifies the chat api while offering streaming as a progressive enhancement


--- 

# Example workflow

1. The user is on the notes page, and asks the chat UI to help them make a post
2. The AI notices the user isn't on the right page based on the included context, and uses a client side tool call to navigate to the calendar
3. The AI asks about the post (when it should be scheduled, and what it should be about)
4. The user gives details about the post
5. The AI searches the user's notes for relevant info about the topic
6. The AI uses the create post client tool to create a post on that date and open the post editor
7. The AI uses the generate caption tool call to create a post matching what the user asked for
8. The user approves the caption, and it gets applied to the post
9. The AI states the task is complete, and asks how else it can help

## LLMOps

* The service is instrumented with LangFuse, an open source LangSmith alternative
    * Centralized Prompt Management, Experiments, Evals
    * Cost and Performance Observability
    * Collecting user feedback
* Models are managed in a central file, organized by functionality (Eg "CreativeModel")
* The service will be deployed in an ECS cluster  